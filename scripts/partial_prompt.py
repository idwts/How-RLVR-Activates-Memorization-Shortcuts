"""
Partial Prompt Completion Evaluation

Evaluates model's ability to complete partial prompts by:
1.Generating partial prompts at different ratios
2.Completing the remaining text
3.Calculating ROUGE-L scores
4.Verifying answer correctness
"""

import json
import os
import sys
from typing import List, Dict
import torch
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams
from rouge_score import rouge_scorer


# Configuration
CONFIG = {
    'model_path': '',  # To be set via command line or here
    'datasets': 'MATH-500,LiveMathBench,MinervaMath',  # Comma-separated
    'prompt_ratios': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],
    'temperature': 0.7,
    'output_dir': 'outputs/partial_prompt',
    'model_prefix': 'model',
    'max_tokens':  1024,
    'redundancy':  1.1,
    'gpu_memory_utilization': 0.8,
}

BENCHMARKS = {
    "MATH-500": {
        "path": "../data/MATH-TTT/test.json",
        "rollouts": 1,
    },
    "LiveMathBench": {
        "path":  "../data/LiveMathBench/livemathbench_2504_v2.json",
        "rollouts": 4,
    },
    "MinervaMath": {
        "path": "../data/MinervaMath/minervamath.json",
        "rollouts":  2,
    }
}


def load_dataset(path: str) -> List[Dict]:
    """Load dataset from file"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def generate_partial_prompt(question: str, ratio: float) -> str:
    """Generate partial prompt, keeping ratio proportion of the question"""
    cutoff_idx = int(len(question) * ratio)
    return question[:cutoff_idx]


def compute_rouge_l(reference: str, prediction: str) -> float:
    """Calculate ROUGE-L score"""
    scorer = rouge_scorer.RougeScorer(["rougeL"], use_stemmer=True)
    score = scorer.score(reference, prediction)
    return score["rougeL"].fmeasure


def extract_answer(prediction: str) -> str:
    """Extract answer from generated result"""
    import re
    match = re.search(r"(\d+)", prediction)
    if match:
        return match.group(1)
    return None


def truncate_generated_output(generated_text: str, remaining_text: str, redundancy: float = 1.5) -> str:
    """
    Truncate generated text based on length of remaining text, allowing some redundancy.
    
    Args:
        generated_text: Complete text generated by model
        remaining_text:  Remaining part of original question
        redundancy: Redundancy coefficient (1.5 means allow 50% extra length)
    
    Returns:
        Truncated generated content
    """
    max_length = int(len(remaining_text) * redundancy)
    return generated_text[:max_length]


def evaluate_partial_prompt(test_data: List[Dict], model:  LLM, tokenizer, prompt_ratios: List[float],
                            sampling_params: SamplingParams, rollouts: int) -> List[Dict]:
    """
    Partial prompt evaluation:  complete text, calculate ROUGE-L and verify answer based on auto_verify
    """
    results = []
    
    for idx, item in enumerate(test_data):
        # Extract question ID (if available)
        question_id = item.get("id", item.get("question_id", idx))
        
        for ratio in prompt_ratios: 
            print(f"Running Partial Prompt Evaluation for Ratio:  {ratio}")
            question = item["prompt"]
            ground_answer = str(item["answer"])
            
            # Generate the Partial Prompt
            partial_prompt = generate_partial_prompt(question, ratio)
            remaining_text = question[len(partial_prompt):]
            
            # Input to model (rollouts are expanded)
            partial_prompts = [partial_prompt] * rollouts
            
            # Run generation
            outputs = model.generate(partial_prompts, sampling_params)
            completion_outputs = [output.outputs[0].text for output in outputs]
            
            # Truncate generated text for ROUGE-L comparison
            truncated_completions = [
                truncate_generated_output(comp, remaining_text, redundancy=CONFIG['redundancy']) 
                for comp in completion_outputs
            ]
            
            # ROUGE-L comparison
            rouge_scores = [compute_rouge_l(remaining_text, comp) for comp in truncated_completions]
            
            # Check answers (using auto_verify)
            sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
            from ttrl.verifier.auto_verify import auto_verify
            auto_verify_results = auto_verify("math", 1, outputs, [ground_answer] * rollouts)
            
            # Evaluate
            rouge_l_avg = sum(rouge_scores) / len(rouge_scores) if rouge_scores else 0.0
            answer_accuracy = sum(auto_verify_results) / len(auto_verify_results)
            
            # Record results
            results.append({
                "question_id": question_id,
                "partial_ratio": ratio,
                "partial_prompt": partial_prompt,
                "remaining_text": remaining_text,
                "ground_truth_answer": ground_answer,
                "generated_outputs": completion_outputs,
                "truncated_for_rouge": truncated_completions,
                "rouge_l_avg": rouge_l_avg,
                "answer_accuracy":  answer_accuracy,
                "matches": auto_verify_results,
            })
    return results


def process(model_path: str, datasets: str, output_dir: str, model_prefix: str) -> None:
    """Evaluates Partial Prompt Completion"""
    # Load Model and Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    model = LLM(
        model_path,
        gpu_memory_utilization=CONFIG['gpu_memory_utilization']
    )
    
    # Sampling Params
    sampling_params = SamplingParams(
        temperature=CONFIG['temperature'],
        max_tokens=CONFIG['max_tokens'],
        include_stop_str_in_output=False,
    )
    
    # Process Each Dataset
    for dataset_name in datasets.split(","):
        if dataset_name not in BENCHMARKS:
            print(f"Warning: Dataset {dataset_name} not found.")
            continue
        
        print(f"Evaluating Dataset: {dataset_name}...")
        
        dataset_info = BENCHMARKS[dataset_name]
        rollouts = dataset_info["rollouts"]
        dataset_path = dataset_info["path"]
        
        # Load test data
        test_data = load_dataset(dataset_path)
        
        # Run Partial Prompt Completion
        ratios = CONFIG['prompt_ratios']
        results = evaluate_partial_prompt(test_data, model, tokenizer, ratios, sampling_params, rollouts)
        
        # Save Results
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{model_prefix}_{dataset_name}_partial_prompt.json")
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=4, ensure_ascii=False)
        
        print(f"Results saved to: {output_file}")


def main():
    """Main entry point - can be called programmatically or modified for command line use"""
    # Example usage:  set these values directly or parse from command line
    model_path = CONFIG.get('model_path', '')
    if not model_path:
        print("Error: model_path must be set in CONFIG")
        return
    
    datasets = CONFIG.get('datasets', 'MATH-500')
    output_dir = CONFIG.get('output_dir', 'outputs/partial_prompt')
    model_prefix = CONFIG.get('model_prefix', 'model')
    
    process(model_path, datasets, output_dir, model_prefix)


if __name__ == "__main__":
    # For command line usage, you can modify this section to parse arguments
    # or simply set values in CONFIG dictionary above
    main()